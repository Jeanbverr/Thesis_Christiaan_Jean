{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5 is not supported on this machine (please install/reinstall h5py for optimal experience)\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "\n",
    "# parser is COMMENTED out because it doesn't work in jupyter notebook!!\n",
    "\n",
    "########## START UNCOMMENT #################\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "# parser.add_argument('--condor',type=int, help=\"increase output verbosity\",default = 0) \n",
    "# parser.add_argument('--ID',type=int,default = 404,\n",
    "#                    help='the ID of the model and the logs of the dexpression run)')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# print(\"ID = \" , args.ID)\n",
    "# print(\"condor = \" , args.condor)\n",
    "\n",
    "\n",
    "# import sys\n",
    "# if (args.condor > 0):\n",
    "#     print (\"Condor turned on\")\n",
    "#     sys.path.insert(0, '/esat/tiger/joramas/mscStudentsData/emotionModeling/libs/python2.7/site-packages')\n",
    "#     sys.path.insert(1,'/esat/tiger/joramas/mscStudentsData/emotionModeling/libs/cuda_9.0/var/cuda-repo-9-0-local/usr/local/cuda-9.0/lib64')\n",
    "#     sys.path.insert(2,'/esat/tiger/joramas/mscStudentsData/emotionModeling/libs/cudnn7/cudnn-9.0-linux-x64-v7/cuda/lib64')\n",
    "\n",
    "#     print(sys.path)\n",
    "\n",
    "# Give a run ID here. Change it to flags (arguments) in version 2.\n",
    "ID = '4_1'\n",
    "\n",
    "# if args.ID == None:\n",
    "#     ID = '4_1'\n",
    "# else:\n",
    "#     ID = repr(args.ID)\n",
    "#     print(args.ID)\n",
    "\n",
    "########## STOP UNCOMMENT #################\n",
    "\n",
    "RUNID = 'DeXpression_run_' + ID\n",
    "\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tflearn.activations as activations\n",
    "# Data loading and preprocessing\n",
    "from tflearn.activations import relu\n",
    "from tflearn.data_utils import shuffle, to_categorical\n",
    "from tflearn.layers.conv import avg_pool_2d, conv_2d, max_pool_2d, global_avg_pool \n",
    "from tflearn.layers.core import dropout, flatten, fully_connected, input_data\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.normalization import batch_normalization\n",
    "\n",
    "\n",
    "#chris library imports\n",
    "# from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from test_recursive_image_load_V2 import load_CKP_data\n",
    "from test_recursive_image_load_V2 import load_formated_data\n",
    "from test_recursive_image_load_V2 import split_dataset\n",
    "from test_recursive_image_load_V2 import divide_subjects\n",
    "from test_recursive_image_load_V2 import divide_data_to_subject\n",
    "from test_recursive_image_load_V2 import load_npy_files\n",
    "\n",
    "from showNumpyInfo import showInfo\n",
    "\n",
    "from Dexpression_network import create_Dexpression_network\n",
    "from Dexpression_network import create_Dexpression_GAP_network\n",
    "from Dexpression_network import create_original_Dexpression_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# global Paths to define for each specific computer\n",
    "#tf_checkpoints = where the checkpoints of tensorflow training algorithms are stored to be recovered if necessary\n",
    "tf_checkpoints = \"G:/Documenten/personal/school/MaNaMA_AI/thesis/implementation/dexpression/github_1/github/Thesis_Christiaan_Jean/Custom_Dexpression/tf_checkpoints\"\n",
    "\n",
    "# cascPath = the path to the cascade file for the facerecognition (relative paths didn't work on my windows edition)\n",
    "cascPath = \"G:/Documenten/personal/school/MaNaMA_AI/thesis/implementation/dexpression/github_1/github/Thesis_Christiaan_Jean/Custom_Dexpression/haarcascade.xml\"\n",
    "\n",
    "# Give a dropout if required (change to True and define the dropout percentage).\n",
    "dropout_keep_prob=0.5\n",
    "\n",
    "# Load data from: https://drive.google.com/drive/folders/1YWT8DJivNOZzQRPCiHDPY0LL_dymdQIS?usp=sharing\n",
    "# X_data = np.load('../data/CKP_X.npy')\n",
    "# Y_data = np.load('../data/CKP_Y.npy')\n",
    "# X_subID = (np.load('../data/CKP_subjectIds.npy')).astype('uint8')\n",
    "\n",
    "# X_data = np.load('../data/CKP_X_all.npy')\n",
    "# Y_data = np.load('../data/CKP_Y_all.npy')\n",
    "# X_subID = (np.load('../data/CKP_subjectIds_all.npy')).astype('uint8')\n",
    "\n",
    "# load_dir = 'CKP_all_neutral'\n",
    "\n",
    "# [X_data,Y_data,X_subID] = load_npy_files(5,load_dir)\n",
    "# data = [X_data,Y_data,X_subID]\n",
    "\n",
    "# print(\"CHECK data type and shape\")\n",
    "\n",
    "# print(\"Type of data var \", type(data))\n",
    "# print(\"Type of data[0] \",  type(data[0]))\n",
    "\n",
    "# showInfo(X_data,\"X_data\")\n",
    "# showInfo(Y_data,\"Y_data\")\n",
    "# showInfo(X_subID ,\"X_subID\")\n",
    "# print(\"-----DONE DEBUG---------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot length  327\n",
      "Maximum memory usage: [211.64453125, 211.64453125, 211.64453125, 211.64453125, 211.64453125]\n"
     ]
    }
   ],
   "source": [
    "#load the subject distribution over the different datasets\n",
    "# subID = (np.load('data_division/train_subject_ID.npy')).astype('uint8')\n",
    "# subID_val = (np.load('data_division/validation_subject_ID.npy')).astype('uint8')\n",
    "# subID_test = (np.load('data_division/test_subject_ID.npy')).astype('uint8')\n",
    "\n",
    "\n",
    "# subID = (np.load('./data_division/'+load_dir+'/train_subject_ID.npy')).astype('uint8')\n",
    "# subID_val = (np.load('./data_division/'+load_dir+'/validation_subject_ID.npy')).astype('uint8')\n",
    "# subID_test = (np.load('./data_division/'+load_dir+'/test_subject_ID.npy')).astype('uint8')\n",
    "# subIDs = [subID, subID_val, subID_test]\n",
    "\n",
    "# print(\"CHECK subID type and shape\")\n",
    "\n",
    "# print(\"Type of data var \", type(data))\n",
    "# print(\"Type of data[0] \",  type(data[0]))\n",
    "# print(\"Type of subIDs var \", type(subIDs))\n",
    "# print(\"Type of subIDs[0] \",  type(subIDs[0]))\n",
    "# showInfo(subID,\"subID\")\n",
    "# showInfo(subID_val,\"subID_val\")\n",
    "# showInfo(subID_test ,\"subID_test\")\n",
    "# print('the sum of instances in all the subID arrays', subID.shape[0]+subID_val.shape[0]+subID_test.shape[0] ,' needs to be the same as the first dimension of the X_data ', X_data.shape[0]  )\n",
    "# print(\"-----DONE DEBUG---------\")7\n",
    "\n",
    "load_dir = 'CKP'\n",
    "\n",
    "X_train = np.load('./generation/predivided_data/'+load_dir+'/X_train.npy')\n",
    "Y_train = np.load('./generation/predivided_data/'+load_dir+'/Y_train.npy')\n",
    "\n",
    "X_val = np.load('./generation/predivided_data/'+load_dir+'/X_val.npy')\n",
    "Y_val = np.load('./generation/predivided_data/'+load_dir+'/Y_val.npy')\n",
    "\n",
    "X_test = np.load('./generation/predivided_data/'+load_dir+'/X_test.npy')\n",
    "Y_test = np.load('./generation/predivided_data/'+load_dir+'/Y_test.npy')\n",
    "\n",
    "print( \"tot length \", len(X_train) + len(X_val) + len(X_test))\n",
    "\n",
    "data = [X_train , Y_train, X_val, Y_val, X_test, Y_test]\n",
    "\n",
    "\n",
    "\n",
    "#monitor the memory usage of the current proces for 1 second, sample each 0.2 s return list of MB values\n",
    "mem_usage = memory_usage(-1, interval=.2, timeout=1)\n",
    "print('Maximum memory usage: %s' % mem_usage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# def logprint(stre):\n",
    "# \tprint(stre)\n",
    "# \tlogging.info(stre)\n",
    "\n",
    "# def getTimeSince(start,str):\n",
    "#     end = time.time()\n",
    "#     print(str)\n",
    "#     print(\"time(sec) \",end - start)\n",
    "#     return end\n",
    "    \n",
    "# def printInfo(i, text,start):   \n",
    "#     if(i%50 == 0):\n",
    "#         logprint(text)\n",
    "#         stre =\" \"+ repr(i) + \" images have been processed\"\n",
    "#         logprint(stre)\n",
    "#         #monitor the memory usage of the current proces for 1 second, sample each 0.2 s return list of MB values\n",
    "#         mem_usage = memory_usage(-1, interval=.2, timeout=1)\n",
    "#         str = 'Maximum memory usage: ' + repr( mem_usage)\n",
    "#         logprint(str)\n",
    "#         getTimeSince(start,text)\n",
    "#         return end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name  X_train\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (205, 224, 224, 1)\n",
      "Name  Y_train\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (205, 7)\n",
      "--------------------------\n",
      "Name  X_val\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (103, 224, 224, 1)\n",
      "Name  Y_val\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (103, 7)\n",
      "--------------------------\n",
      "Name  X_test\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (19, 224, 224, 1)\n",
      "Name  Y_test\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (19, 7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# start = time.time()\n",
    "# print(\"dividing the data\")\n",
    "# divided_data = divide_data_to_subject(data,subIDs)\n",
    "\n",
    "# X = (divided_data[0].reshape(-1,224,224,1)).astype('uint8')\n",
    "# Y = (divided_data[1].reshape(-1,7)).astype('uint8')\n",
    "\n",
    "# # create the validation set X_val and Y-val (SubID_val is not given to the network)\n",
    "# X_val = divided_data[2].reshape(-1,224,224,1).astype('uint8')\n",
    "# Y_val = divided_data[3].reshape(-1,7).astype('uint8')\n",
    "\n",
    "# # create the test set X_test and Y_test (SubID_test is not given to the network)\n",
    "# X_test = divided_data[4].reshape(-1,224,224,1).astype('uint8')\n",
    "# Y_test = divided_data[5].reshape(-1,7).astype('uint8')\n",
    "\n",
    "showInfo(X_train,\"X_train\")\n",
    "showInfo(Y_train,\"Y_train\")\n",
    "print('--------------------------')\n",
    "showInfo(X_val,\"X_val\")\n",
    "showInfo(Y_val,\"Y_val\")\n",
    "print('--------------------------')\n",
    "showInfo(X_test,\"X_test\")\n",
    "showInfo(Y_test,\"Y_test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# seperate the dataset in 11 parts that do not share users: \n",
    "#   9 parts train set\n",
    "#   1 part validation set\n",
    "#   1 part test set\n",
    "# allows future possibility for 10-fold crossover\n",
    "\n",
    "# parts = split_dataset([X_reduced,Y_reduced,X_subID],11)\n",
    "\n",
    "# X_parts = parts[0]\n",
    "# Y_parts = parts[1]\n",
    "# X_subID_parts = parts[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select 9 from the 11 parts (part 11 is usually the smallest so it is bad as test or validation data)\n",
    "# select =  [0,1,2,3,6,7,8,9,10]\n",
    "# select_val = [4]\n",
    "# select_test= [5]\n",
    "\n",
    "# selection = [select, select_val, select_test]\n",
    "\n",
    "# if((len(select)+1 + 1)!= len(X_parts)):\n",
    "#     print(\"Warning it is possible that not all parts of the dataset are used\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the subjects from a database according to the division in selection\n",
    "# IN:\n",
    "# X_subID_parts = the subject labels of the data set split in to N parts\n",
    "# selection = a list with 3 lists [select select_val select_test]\n",
    "# OUT:\n",
    "# A list of 3 lists with subject numbers for training, validation and test sets\n",
    "# subIDs =  divide_subjects(X_subID_parts,selection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_predivided_dexpression_model(RUNID,data,tf_checkpoints,cascPath,dropout_keep_prob=0.5):\n",
    "    \n",
    "    print(\"CHECK data types and shapes\")\n",
    "    print(\"length of data list: \",len(data))\n",
    "    print(\"Type of data var \", type(data))\n",
    "    print(\"Type of data[0] \",  type(data[0]))\n",
    "\n",
    "    \n",
    "    showInfo(data[0],\"X_train\")\n",
    "    showInfo(data[1],\"Y_train\")\n",
    "    showInfo(data[2],\"X_val\")\n",
    "    showInfo(data[3],\"Y_val\")\n",
    "    showInfo(data[4],\"X_test\")\n",
    "    showInfo(data[5],\"Y_test\")\n",
    "\n",
    "    print(\"-----DONE DEBUG---------\")\n",
    "    \n",
    "\n",
    "    X = (data[0].reshape(-1,224,224,1)).astype('uint8')\n",
    "    Y = (data[1].reshape(-1,7)).astype('uint8')\n",
    "\n",
    "    # create the validation set X_val and Y-val (SubID_val is not given to the network)\n",
    "    X_val = data[2].reshape(-1,224,224,1).astype('uint8')\n",
    "    Y_val = data[3].reshape(-1,7).astype('uint8')\n",
    "\n",
    "    # create the test set X_test and Y_test (SubID_test is not given to the network)\n",
    "    X_test = data[4].reshape(-1,224,224,1).astype('uint8')\n",
    "    Y_test = data[5].reshape(-1,7).astype('uint8')\n",
    "\n",
    "#     network = create_Dexpression_network(dropout_keep_prob)\n",
    "\n",
    "#     network with GAP layer\n",
    "    network = create_original_Dexpression_network(dropout_keep_prob)\n",
    "    \n",
    "\n",
    "\n",
    "    #create a custom tensorflow session to manage the used resources\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.Session(config = config)\n",
    "\n",
    "\n",
    "    # Final definition of model checkpoints and other configurations\n",
    "    #model = tflearn.DNN(network, checkpoint_path='/home/cc/DeXpression/DeXpression_checkpoints',\n",
    "    model = tflearn.DNN(network, checkpoint_path=tf_checkpoints,\n",
    "                        max_checkpoints=1, tensorboard_verbose=2, tensorboard_dir=\"./tflearn_logs/\")\n",
    "\n",
    "\n",
    "    # Fit the model, train for 20 epochs. (Change all parameters to flags (arguments) on version 2.)\n",
    "    #model.fit(X, Y, n_epoch=20, validation_set=0.1, shuffle=True, show_metric=True, batch_size=50, snapshot_step=2000,snapshot_epoch=True, run_id=RUNID)\n",
    "    model.fit(X, Y, n_epoch=20, validation_set=(X_val,Y_val), shuffle=True, show_metric=True, batch_size=50, snapshot_step=2000,snapshot_epoch=True, run_id=RUNID)\n",
    "\n",
    "    # Save the model\n",
    "    model.save(tf_checkpoints + '/' + RUNID + '.model')\n",
    "    print(\"finished training and saving\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m0.73722\u001b[0m\u001b[0m | time: 7.756s\n",
      "| Adam | epoch: 020 | loss: 0.73722 - acc: 0.8322 -- iter: 200/205\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m1.06384\u001b[0m\u001b[0m | time: 11.031s\n",
      "| Adam | epoch: 020 | loss: 1.06384 - acc: 0.7710 | val_loss: 3.46340 - val_acc: 0.4175 -- iter: 205/205\n",
      "--\n",
      "finished training and saving\n"
     ]
    }
   ],
   "source": [
    "train_on_predivided_dexpression_model(RUNID,data,tf_checkpoints,cascPath,dropout_keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the array with the Id's of each user per set, for future reference\n",
    "# np.save('data_division/train_subject_ID.npy',subID)\n",
    "# np.save('data_division/validation_subject_ID.npy',subID_val)\n",
    "# np.save('data_division/test_subject_ID.npy',subID_test)\n",
    "\n",
    "\n",
    "# just test outpt\n",
    "\n",
    "# showInfo(X,\"X\")\n",
    "# showInfo(Y,\"Y\")\n",
    "# showInfo(X_val,\"X_vall\")\n",
    "# showInfo(Y_val,\"Y_vall\")\n",
    "# showInfo(X_test,\"X_test\")\n",
    "# showInfo(Y_test,\"Y_test\")\n",
    "\n",
    "# for i in range(0,len(X)):\n",
    "#     str = \"i \" + repr(i) + \" emo \" +  repr(Y[i]) + \" X  \"+ repr(subID[i]) \n",
    "#     print(str)\n",
    "#     cv2.imshow(str, X[i].reshape((224,224)))\n",
    "#     cv2.namedWindow(str,cv2.WINDOW_NORMAL)\n",
    "#     cv2.resizeWindow(str, 600,600)\n",
    "#     cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showInfo(X , \"X\")\n",
    "\n",
    "# cv2.imshow(\"example\", X[1].reshape((224,224)))\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "# print(\"size labels shape: \" + repr(Y.shape))\n",
    "# print(\"type labels: \" + repr(type(Y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_Dexpression_network():\n",
    "#     # Define number of output classes.\n",
    "#     num_classes = 7\n",
    "\n",
    "#     # Define padding scheme.\n",
    "#     padding = 'VALID'\n",
    "\n",
    "#     # Model Architecture\n",
    "#     network = input_data(shape=[None, 224, 224, 1])\n",
    "#     conv_1 = relu(conv_2d(network, 64, 7, strides=2, bias=True, padding=padding, activation=None, name='Conv2d_1'))\n",
    "#     maxpool_1 = batch_normalization(max_pool_2d(conv_1, 3, strides=2, padding=padding, name='MaxPool_1'))\n",
    "#     LRN_1 = local_response_normalization(maxpool_1, name='LRN_1')\n",
    "#     # FeatEX-1\n",
    "#     conv_2a = relu(conv_2d(maxpool_1, 96, 1, strides=1, padding=padding, name='Conv_2a_FX1'))\n",
    "#     maxpool_2a = max_pool_2d(maxpool_1, 3, strides=1, padding=padding, name='MaxPool_2a_FX1')\n",
    "#     conv_2b = relu(conv_2d(conv_2a, 208, 3, strides=1, padding=padding, name='Conv_2b_FX1'))\n",
    "#     conv_2c = relu(conv_2d(maxpool_2a, 64, 1, strides=1, padding=padding, name='Conv_2c_FX1'))\n",
    "#     FX1_out = merge([conv_2b, conv_2c], mode='concat', axis=3, name='FX1_out')\n",
    "#     # FeatEX-2\n",
    "#     conv_3a = relu(conv_2d(FX1_out, 96, 1, strides=1, padding=padding, name='Conv_3a_FX2'))\n",
    "#     maxpool_3a = max_pool_2d(FX1_out, 3, strides=1, padding=padding, name='MaxPool_3a_FX2')\n",
    "#     conv_3b = relu(conv_2d(conv_3a, 208, 3, strides=1, padding=padding, name='Conv_3b_FX2'))\n",
    "#     conv_3c = relu(conv_2d(maxpool_3a, 64, 1, strides=1, padding=padding, name='Conv_3c_FX2'))\n",
    "#     FX2_out = merge([conv_3b, conv_3c], mode='concat', axis=3, name='FX2_out')\n",
    "#     net = flatten(FX2_out)\n",
    "#     if dropout:\n",
    "#         net = dropout(net, dropout_keep_prob)\n",
    "#     loss = fully_connected(net, num_classes,activation='softmax')\n",
    "\n",
    "#     # Compile the model and define the hyperparameters\n",
    "#     network = tflearn.regression(loss, optimizer='Adam',\n",
    "#                          loss='categorical_crossentropy',\n",
    "#                          learning_rate=0.0001)\n",
    "#     return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "report_tensor_allocations_upon_oom: true"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.RunOptions(report_tensor_allocations_upon_oom = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cv2.imshow(\"example\", X[1].reshape((224,224)))\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "\n",
    "# Load the model if required, later.\n",
    "#model.load('./DeXpression_checkpoints/' + RUNID + '.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
