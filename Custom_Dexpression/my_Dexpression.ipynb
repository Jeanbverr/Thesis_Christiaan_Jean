{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5 is not supported on this machine (please install/reinstall h5py for optimal experience)\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tflearn\n",
    "import tflearn.activations as activations\n",
    "# Data loading and preprocessing\n",
    "from tflearn.activations import relu\n",
    "from tflearn.data_utils import shuffle, to_categorical\n",
    "from tflearn.layers.conv import avg_pool_2d, conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import dropout, flatten, fully_connected, input_data\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.normalization import batch_normalization\n",
    "\n",
    "\n",
    "#chris library imports\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from test_recursive_image_load_V2 import load_CKP_data\n",
    "from test_recursive_image_load_V2 import load_formated_data\n",
    "from showNumpyInfo import showInfo\n",
    "\n",
    "\n",
    "# global Paths to define for each specific computer\n",
    "#tf_checkpoints = where the checkpoints of tensorflow training algorithms are stored to be recovered if necessary\n",
    "tf_checkpoints = \"G:/Documenten/personal/school/MaNaMA_AI/thesis/implementation/dexpression/github_1/github/Thesis_Christiaan_Jean/Custom_Dexpression/tf_checkpoints\"\n",
    "\n",
    "# cascPath = the path to the cascade file for the facerecognition (relative paths didn't work on my windows edition)\n",
    "cascPath = \"G:/Documenten/personal/school/MaNaMA_AI/thesis/implementation/dexpression/github_1/github/Thesis_Christiaan_Jean/Custom_Dexpression/haarcascade.xml\"\n",
    "\n",
    "# Give a run ID here. Change it to flags (arguments) in version 2.\n",
    "ID = '4_2'\n",
    "RUNID = 'DeXpression_run_' + ID\n",
    "\n",
    "# Give a dropout if required (change to True and define the dropout percentage).\n",
    "dropout = False\n",
    "dropout_keep_prob=0.5\n",
    "\n",
    "# Load data from: https://drive.google.com/drive/folders/1YWT8DJivNOZzQRPCiHDPY0LL_dymdQIS?usp=sharing\n",
    "X_data = np.load('CKP_X.npy')\n",
    "Y_data = np.load('CKP_Y.npy')\n",
    "X_subID = (np.load('CKP_subjectIds.npy')).astype('uint8')\n",
    "\n",
    "X_reduced = X_data.astype('uint8')\n",
    "Y_reduced = Y_data\n",
    "\n",
    "# showInfo(X_reduced,\"X_reduced\")\n",
    "# showInfo(Y_reduced,\"Y_reduced\")\n",
    "\n",
    "# showInfo(X_subID,\"X_subID\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5  10  10  10  11  11  11  11  11  11  14  14  14  14  22  22  22  26\n",
      "  26  26  26  28  29  32  32  32  32  32  34  34  34  35  35  35  37  37\n",
      "  37  42  42  42  42  44  44  44  45  45  46  46  46  46  50  50  50  50\n",
      "  51  51  52  52  52  53  53  54  54  54  55  55  55  55  55  56  56  56\n",
      "  57  57  57  58  58  58  59  59  60  60  60  61  61  61  62  62  62  62\n",
      "  63  63  64  64  64  65  65  65  65  66  66  66  66  67  67  67  67  68\n",
      "  68  68  68  69  69  69  70  70  70  71  71  71  71  71  72  72  73  73\n",
      "  74  74  74  74  75  75  75  75  76  76  76  77  77  78  78  78  79  79\n",
      "  79  80  80  80  81  81  81  82  82  82  83  84  84  85  85  85  86  86\n",
      "  87  87  87  87  88  88  89  89  89  90  90  90  91  91  92  92  92  93\n",
      "  93  94  94  95  95  95  95  96  96  96  97  97  97  98  98  99  99  99\n",
      " 100 100 100 101 102 102 102 105 106 106 106 107 107 108 108 108 109 109\n",
      " 109 110 111 111 111 112 113 113 113 114 114 115 115 115 116 116 116 117\n",
      " 117 117 119 119 119 122 124 124 124 124 125 125 125 125 125 126 126 127\n",
      " 127 127 128 128 129 129 129 129 130 130 130 130 130 131 131 131 131 132\n",
      " 132 132 132 132 133 133 133 134 134 134 135 135 136 136 136 136 137 137\n",
      " 137 138 138 138 138 138 139 147 148 149 151 154 155 156 157 158 160 245\n",
      " 245 245 246 246 246 247 247 247 248 248 248 248 249 249 250 250 250 250\n",
      " 127 231 231]\n"
     ]
    }
   ],
   "source": [
    "print(X_subID)\n",
    "\n",
    "# seperate the dataset in 11 parts that do not share users: \n",
    "#   9 parts train set\n",
    "#   1 part validation set\n",
    "#   1 part test set\n",
    "# allows future possibility for 10-fold crossover\n",
    "\n",
    "# X_parts = np.asarray([np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]),np.asarray([]), np.asarray([])])\n",
    "X_parts = []\n",
    "# print(type(X_parts))\n",
    "# print(type(X_parts[0]))\n",
    "# print(len(X_parts))\n",
    "# print(X_parts)\n",
    "# print(X_parts[0])\n",
    "\n",
    "# Y_parts =  np.asarray([np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]),np.asarray([]), np.asarray([])])\n",
    "# X_subID_parts =  np.asarray([np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]),np.asarray([]), np.asarray([])])\n",
    "Y_parts = []\n",
    "X_subID_parts = []  \n",
    "                         \n",
    "# showInfo(X_parts ,\"X_parts\")\n",
    "# showInfo(Y_parts ,\"Y_parts\")    \n",
    "    \n",
    "X_subID_unique,X_subID_unique_ID,X_subID_count  = np.unique(X_subID,return_index= True,return_counts=True)\n",
    "X_subID_cum = np.cumsum(X_subID_count)\n",
    "# print(\"X_subID_unique\")\n",
    "# print(X_subID_unique)\n",
    "# print(\"X_subID_unique_ID\")\n",
    "# print(X_subID_unique_ID)\n",
    "# print(\"X_subID_count\")\n",
    "# print(X_subID_count)\n",
    "# print(\"X_subID_cum\")  \n",
    "# print(X_subID_cum)\n",
    "\n",
    "# cv2.imshow(\"example\", X_reduced[0].reshape((224,224)))\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partsize  27\n",
      "11\n",
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "partSize = (np.floor(len((X_subID))/12)).astype('uint8')\n",
    "print(\"partsize \" , partSize)\n",
    "\n",
    "lastSubID = 0 #the ID of the subject of the last instance\n",
    "partCount = 0 #The amount of instances in this part already\n",
    "partID = 0    #keeps the number of the part that is being filled right now\n",
    "\n",
    "X_array = np.asarray([])\n",
    "Y_array = np.asarray([])\n",
    "sub_array = np.asarray([])\n",
    "\n",
    "X_parts = []\n",
    "# print(type(X_parts))\n",
    "# print(type(X_parts[0]))\n",
    "# print(len(X_parts))\n",
    "# print(X_parts)\n",
    "# print(X_parts[0])\n",
    "\n",
    "# Y_parts =  np.asarray([np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]),np.asarray([]), np.asarray([])])\n",
    "# X_subID_parts =  np.asarray([np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]), np.asarray([]),np.asarray([]), np.asarray([])])\n",
    "Y_parts = []\n",
    "X_subID_parts = []  \n",
    "\n",
    "#iterate through al the ID's if a part is full and the subjectID is different from the previous, fill the next part \n",
    "for i in range(0,len(X_subID)):\n",
    "\n",
    "#     print(\"this part IDs top border \", partSize ,\" and partCount = \", partCount)\n",
    "    if((partCount >= partSize) & (lastSubID != X_subID[i])):\n",
    "      \n",
    "        \n",
    "        #fill the parts to the partslists\n",
    "        X_parts.append(X_array)\n",
    "        Y_parts.append(Y_array)\n",
    "        X_subID_parts.append(sub_array)\n",
    "        \n",
    "        #refresh the temporary arrays\n",
    "        X_array = np.asarray([])\n",
    "        Y_array = np.asarray([])\n",
    "        sub_array = np.asarray([])\n",
    "        #reset counter & set next part to be filled\n",
    "        partCount = 0\n",
    "        partID = partID + 1\n",
    "        \n",
    "        \n",
    "    X_array = np.append(X_array ,X_reduced[i])\n",
    "    Y_array = np.append(Y_array ,Y_reduced[i])\n",
    "    sub_array = np.append(sub_array ,X_subID[i])  \n",
    "    \n",
    "\n",
    "    partCount = partCount +1\n",
    "    lastSubID = X_subID[i]\n",
    "\n",
    "print(len(X_parts))\n",
    "print(len(Y_parts))\n",
    "print(len(X_subID_parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (28, 224, 224, 1)\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (28, 7)\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (28,)\n",
      "1\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (28, 224, 224, 1)\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (28, 7)\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (28,)\n",
      "2\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (27, 224, 224, 1)\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (27, 7)\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (27,)\n",
      "3\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (28, 224, 224, 1)\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (28, 7)\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (28,)\n",
      "4\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (28, 224, 224, 1)\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (28, 7)\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (28,)\n",
      "5\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (27, 224, 224, 1)\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (27, 7)\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (27,)\n",
      "6\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (27, 224, 224, 1)\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (27, 7)\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (27,)\n",
      "7\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (28, 224, 224, 1)\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (28, 7)\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (28,)\n",
      "8\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (28, 224, 224, 1)\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (28, 7)\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (28,)\n",
      "9\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (28, 224, 224, 1)\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (28, 7)\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (28,)\n",
      "10\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (27, 224, 224, 1)\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (27, 7)\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  float64\n",
      "shape:  (27,)\n"
     ]
    }
   ],
   "source": [
    "# print(len(X_subID))\n",
    "# print(\"partsize\", partSize)\n",
    "\n",
    "for o in range(0,len(X_parts)):\n",
    "    print (o)\n",
    "    showInfo(X_parts[o].reshape(-1,224,224,1))\n",
    "    showInfo(Y_parts[o].reshape(-1,7))\n",
    "    showInfo(X_subID_parts[o])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name  X\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (249, 224, 224, 1)\n",
      "Name  Y\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (249, 7)\n",
      "Name  X_vall\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (28, 224, 224, 1)\n",
      "Name  Y_vall\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (28, 7)\n",
      "Name  X_test\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (27, 224, 224, 1)\n",
      "Name  Y_test\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (27, 7)\n"
     ]
    }
   ],
   "source": [
    "#select 9 from the 11 parts (part 11 is usually the smallest so it is bad as test or validation data)\n",
    "select =  [0,1,2,3,6,7,8,9,10]\n",
    "select_val = 4\n",
    "select_test= 5\n",
    "\n",
    "if((len(select)+1 + 1)!= len(X_parts)):\n",
    "    print(\"Warning it is possible that not all parts of the dataset are used\")\n",
    "\n",
    "X = np.asarray([])\n",
    "Y = np.asarray([])\n",
    "subID = np.asarray([])\n",
    "    \n",
    "for i in select:\n",
    "    X = np.append(X,X_parts[i])\n",
    "    Y = np.append(Y,Y_parts[i])\n",
    "    subID = np.append(subID,X_subID_parts[i])\n",
    "\n",
    "X = (X.reshape(-1,224,224,1)).astype('uint8')\n",
    "Y = (Y.reshape(-1,7)).astype('uint8')\n",
    "subID = subID.astype('uint8')\n",
    "    \n",
    "X_val = X_parts[select_val].reshape(-1,224,224,1).astype('uint8')\n",
    "Y_val = Y_parts[select_val].reshape(-1,7).astype('uint8')\n",
    "subID_val = X_subID_parts[select_val].astype('uint8')\n",
    "\n",
    "X_test = X_parts[select_test].reshape(-1,224,224,1).astype('uint8')\n",
    "Y_test = Y_parts[select_test].reshape(-1,7).astype('uint8')\n",
    "subID_test = X_subID_parts[select_test].astype('uint8')\n",
    "\n",
    "showInfo(X,\"X\")\n",
    "showInfo(Y,\"Y\")\n",
    "showInfo(X_val,\"X_vall\")\n",
    "showInfo(Y_val,\"Y_vall\")\n",
    "showInfo(X_test,\"X_test\")\n",
    "showInfo(Y_test,\"Y_test\")\n",
    "\n",
    "# for i in range(0,len(X)):\n",
    "#     str = \"index \" + repr(i) + \" X  \"+ repr(subID[i]) + \" emotion \" +  repr(Y[i])\n",
    "#     print(str)\n",
    "#     cv2.imshow(str, X[i].reshape((224,224)))\n",
    "#     cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name  X\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (249, 224, 224, 1)\n",
      "size labels shape: (249, 7)\n",
      "type labels: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "showInfo(X , \"X\")\n",
    "\n",
    "# cv2.imshow(\"example\", X[1].reshape((224,224)))\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "print(\"size labels shape: \" + repr(Y.shape))\n",
    "print(\"type labels: \" + repr(type(Y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From a:\\users\\christiaan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tflearn\\initializations.py:119: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From a:\\users\\christiaan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tflearn\\objectives.py:66: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# Define number of output classes.\n",
    "num_classes = 7\n",
    "\n",
    "# Define padding scheme.\n",
    "padding = 'VALID'\n",
    "\n",
    "# Model Architecture\n",
    "network = input_data(shape=[None, 224, 224, 1])\n",
    "conv_1 = relu(conv_2d(network, 64, 7, strides=2, bias=True, padding=padding, activation=None, name='Conv2d_1'))\n",
    "maxpool_1 = batch_normalization(max_pool_2d(conv_1, 3, strides=2, padding=padding, name='MaxPool_1'))\n",
    "#LRN_1 = local_response_normalization(maxpool_1, name='LRN_1')\n",
    "# FeatEX-1\n",
    "conv_2a = relu(conv_2d(maxpool_1, 96, 1, strides=1, padding=padding, name='Conv_2a_FX1'))\n",
    "maxpool_2a = max_pool_2d(maxpool_1, 3, strides=1, padding=padding, name='MaxPool_2a_FX1')\n",
    "conv_2b = relu(conv_2d(conv_2a, 208, 3, strides=1, padding=padding, name='Conv_2b_FX1'))\n",
    "conv_2c = relu(conv_2d(maxpool_2a, 64, 1, strides=1, padding=padding, name='Conv_2c_FX1'))\n",
    "FX1_out = merge([conv_2b, conv_2c], mode='concat', axis=3, name='FX1_out')\n",
    "# FeatEX-2\n",
    "conv_3a = relu(conv_2d(FX1_out, 96, 1, strides=1, padding=padding, name='Conv_3a_FX2'))\n",
    "maxpool_3a = max_pool_2d(FX1_out, 3, strides=1, padding=padding, name='MaxPool_3a_FX2')\n",
    "conv_3b = relu(conv_2d(conv_3a, 208, 3, strides=1, padding=padding, name='Conv_3b_FX2'))\n",
    "conv_3c = relu(conv_2d(maxpool_3a, 64, 1, strides=1, padding=padding, name='Conv_3c_FX2'))\n",
    "FX2_out = merge([conv_3b, conv_3c], mode='concat', axis=3, name='FX2_out')\n",
    "net = flatten(FX2_out)\n",
    "if dropout:\n",
    "    net = dropout(net, dropout_keep_prob)\n",
    "loss = fully_connected(net, num_classes,activation='softmax')\n",
    "\n",
    "# Compile the model and define the hyperparameters\n",
    "network = tflearn.regression(loss, optimizer='Adam',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     learning_rate=0.0001)\n",
    "\n",
    "#create a custom tensorflow session to manage the used resources\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config = config)\n",
    "\n",
    "\n",
    "# Final definition of model checkpoints and other configurations\n",
    "#model = tflearn.DNN(network, checkpoint_path='/home/cc/DeXpression/DeXpression_checkpoints',\n",
    "model = tflearn.DNN(network, checkpoint_path=tf_checkpoints,\n",
    "                    max_checkpoints=1, tensorboard_verbose=2, tensorboard_dir=\"./tflearn_logs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "report_tensor_allocations_upon_oom: true"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.RunOptions(report_tensor_allocations_upon_oom = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m1.02537\u001b[0m\u001b[0m | time: 18.071s\n",
      "| Adam | epoch: 020 | loss: 1.02537 - acc: 0.8902 -- iter: 200/249\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m1.74276\u001b[0m\u001b[0m | time: 24.281s\n",
      "| Adam | epoch: 020 | loss: 1.74276 - acc: 0.8231 | val_loss: 0.79043 - val_acc: 0.7407 -- iter: 249/249\n",
      "--\n",
      "finished training and saving\n"
     ]
    }
   ],
   "source": [
    "# Fit the model, train for 20 epochs. (Change all parameters to flags (arguments) on version 2.)\n",
    "#model.fit(X, Y, n_epoch=20, validation_set=0.1, shuffle=True, show_metric=True, batch_size=50, snapshot_step=2000,snapshot_epoch=True, run_id=RUNID)\n",
    "model.fit(X, Y, n_epoch=20, validation_set=(X_test,Y_test), shuffle=True, show_metric=True, batch_size=50, snapshot_step=2000,snapshot_epoch=True, run_id=RUNID)\n",
    "\n",
    "# Save the model\n",
    "model.save(tf_checkpoints + '/' + RUNID + '.model')\n",
    "print(\"finished training and saving\")\n",
    "\n",
    "# Load the model if required, later.\n",
    "#model.load('./DeXpression_checkpoints/' + RUNID + '.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
