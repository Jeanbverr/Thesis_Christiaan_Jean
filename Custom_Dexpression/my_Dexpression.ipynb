{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5 is not supported on this machine (please install/reinstall h5py for optimal experience)\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "# parser is COMMENTED out because it doesn't work in jupyter notebook!!\n",
    "\n",
    "########## START UNCOMMENT #################\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "# parser.add_argument(\"--condor\", help=\"increase output verbosity\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# parser.add_argument('-ID',type=int,\n",
    "#                    help='the ID of the model and the logs of the dexpression run)')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# print(args.ID)\n",
    "\n",
    "\n",
    "import sys\n",
    "# if args.condor:\n",
    "#     print \"Condor turned on\"\n",
    "#     sys.path.insert(0, '/esat/tiger/joramas/mscStudentsData/emotionModeling/libs/python2.7/site-packages')\n",
    "#     sys.path.insert(1,'/esat/tiger/joramas/mscStudentsData/emotionModeling/libs/cuda_9.0/var/cuda-repo-9-0-local/usr/local/cuda-9.0/lib64')\n",
    "#     sys.path.insert(2,'/esat/tiger/joramas/mscStudentsData/emotionModeling/libs/cudnn7/cudnn-9.0-linux-x64-v7/cuda/lib64')\n",
    "\n",
    "#     print(sys.path)\n",
    "\n",
    "# Give a run ID here. Change it to flags (arguments) in version 2.\n",
    "ID = '4_1'\n",
    "\n",
    "# if args.ID == None:\n",
    "#     ID = '4_1'\n",
    "# else:\n",
    "#     ID = repr(args.ID)\n",
    "#     print(args.ID)\n",
    "    \n",
    "RUNID = 'DeXpression_run_' + ID\n",
    "\n",
    "########## STOP UNCOMMENT #################\n",
    "\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tflearn.activations as activations\n",
    "# Data loading and preprocessing\n",
    "from tflearn.activations import relu\n",
    "from tflearn.data_utils import shuffle, to_categorical\n",
    "from tflearn.layers.conv import avg_pool_2d, conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import dropout, flatten, fully_connected, input_data\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.normalization import batch_normalization\n",
    "\n",
    "\n",
    "#chris library imports\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from test_recursive_image_load_V2 import load_CKP_data\n",
    "from test_recursive_image_load_V2 import load_formated_data\n",
    "from test_recursive_image_load_V2 import split_dataset\n",
    "from test_recursive_image_load_V2 import divide_subjects\n",
    "from test_recursive_image_load_V2 import divide_data_to_subject\n",
    "\n",
    "from showNumpyInfo import showInfo\n",
    "\n",
    "from Dexpression_network import create_Dexpression_network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# global Paths to define for each specific computer\n",
    "#tf_checkpoints = where the checkpoints of tensorflow training algorithms are stored to be recovered if necessary\n",
    "tf_checkpoints = \"G:/Documenten/personal/school/MaNaMA_AI/thesis/implementation/dexpression/github_1/github/Thesis_Christiaan_Jean/Custom_Dexpression/tf_checkpoints\"\n",
    "\n",
    "# cascPath = the path to the cascade file for the facerecognition (relative paths didn't work on my windows edition)\n",
    "# cascPath = \"G:/Documenten/personal/school/MaNaMA_AI/thesis/implementation/dexpression/github_1/github/Thesis_Christiaan_Jean/Custom_Dexpression/haarcascade.xml\"\n",
    "\n",
    "\n",
    "\n",
    "# Give a dropout if required (change to True and define the dropout percentage).\n",
    "dropout_keep_prob=0.5\n",
    "\n",
    "# Load data from: https://drive.google.com/drive/folders/1YWT8DJivNOZzQRPCiHDPY0LL_dymdQIS?usp=sharing\n",
    "X_data = np.load('../data/CKP_X.npy')\n",
    "Y_data = np.load('../data/CKP_Y.npy')\n",
    "X_subID = (np.load('../data/CKP_subjectIds.npy')).astype('uint8')\n",
    "\n",
    "X_reduced = X_data.astype('uint8')\n",
    "Y_reduced = Y_data\n",
    "\n",
    "#load the subject distribution over the different datasets\n",
    "subID = (np.load('data_division/train_subject_ID.npy')).astype('uint8')\n",
    "subID_val = (np.load('data_division/validation_subject_ID.npy')).astype('uint8')\n",
    "subID_test = (np.load('data_division/test_subject_ID.npy')).astype('uint8')\n",
    "subIDs = [subID, subID_val, subID_test]\n",
    "\n",
    "# showInfo(X_reduced,\"X_reduced\")\n",
    "# showInfo(Y_reduced,\"Y_reduced\")\n",
    "\n",
    "# showInfo(X_subID,\"X_subID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partsize  27\n",
      "11\n",
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# seperate the dataset in 11 parts that do not share users: \n",
    "#   9 parts train set\n",
    "#   1 part validation set\n",
    "#   1 part test set\n",
    "# allows future possibility for 10-fold crossover\n",
    "\n",
    "# parts = split_dataset([X_reduced,Y_reduced,X_subID],11)\n",
    "\n",
    "# X_parts = parts[0]\n",
    "# Y_parts = parts[1]\n",
    "# X_subID_parts = parts[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select 9 from the 11 parts (part 11 is usually the smallest so it is bad as test or validation data)\n",
    "# select =  [0,1,2,3,6,7,8,9,10]\n",
    "# select_val = [4]\n",
    "# select_test= [5]\n",
    "\n",
    "# selection = [select, select_val, select_test]\n",
    "\n",
    "# if((len(select)+1 + 1)!= len(X_parts)):\n",
    "#     print(\"Warning it is possible that not all parts of the dataset are used\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the subjects from a database according to the division in selection\n",
    "# IN:\n",
    "# X_subID_parts = the subject labels of the data set split in to N parts\n",
    "# selection = a list with 3 lists [select select_val select_test]\n",
    "# OUT:\n",
    "# A list of 3 lists with subject numbers for training, validation and test sets\n",
    "# subIDs =  divide_subjects(X_subID_parts,selection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divides the data in training, validation and test sets according to lists of the subjectIDs already divided over the 3\n",
    "# IN:\n",
    "# data = contain 3 1D-arrays: x,y and subject data [X_data, Y_data,X_subID]\n",
    "# subIDs = contain 3 1D-arrays with the subject numbers for each set: train,val,test [subID subID_val subID_test]\n",
    "# OUT:\n",
    "# list of 6 arrays [X,Y,X_val,Y_val,X_test,Y_test]\n",
    "divided_data = divide_data_to_subject([X_data,Y_data,X_subID],subIDs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (divided_data[0].reshape(-1,224,224,1)).astype('uint8')\n",
    "Y = (divided_data[1].reshape(-1,7)).astype('uint8')\n",
    "\n",
    "# create the validation set X_val and Y-val (SubID_val is not given to the network)\n",
    "X_val = divided_data[2].reshape(-1,224,224,1).astype('uint8')\n",
    "Y_val = divided_data[3].reshape(-1,7).astype('uint8')\n",
    "\n",
    "# create the test set X_test and Y_test (SubID_test is not given to the network)\n",
    "X_test = divided_data[4].reshape(-1,224,224,1).astype('uint8')\n",
    "Y_test = divided_data[5].reshape(-1,7).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the array with the Id's of each user per set, for future reference\n",
    "# np.save('data_division/train_subject_ID.npy',subID)\n",
    "# np.save('data_division/validation_subject_ID.npy',subID_val)\n",
    "# np.save('data_division/test_subject_ID.npy',subID_test)\n",
    "\n",
    "\n",
    "# just test outpt\n",
    "\n",
    "# showInfo(X,\"X\")\n",
    "# showInfo(Y,\"Y\")\n",
    "# showInfo(X_val,\"X_vall\")\n",
    "# showInfo(Y_val,\"Y_vall\")\n",
    "# showInfo(X_test,\"X_test\")\n",
    "# showInfo(Y_test,\"Y_test\")\n",
    "\n",
    "# for i in range(0,len(X)):\n",
    "#     str = \"i \" + repr(i) + \" emo \" +  repr(Y[i]) + \" X  \"+ repr(subID[i]) \n",
    "#     print(str)\n",
    "#     cv2.imshow(str, X[i].reshape((224,224)))\n",
    "#     cv2.namedWindow(str,cv2.WINDOW_NORMAL)\n",
    "#     cv2.resizeWindow(str, 600,600)\n",
    "#     cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showInfo(X , \"X\")\n",
    "\n",
    "# cv2.imshow(\"example\", X[1].reshape((224,224)))\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "print(\"size labels shape: \" + repr(Y.shape))\n",
    "print(\"type labels: \" + repr(type(Y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_Dexpression_network():\n",
    "#     # Define number of output classes.\n",
    "#     num_classes = 7\n",
    "\n",
    "#     # Define padding scheme.\n",
    "#     padding = 'VALID'\n",
    "\n",
    "#     # Model Architecture\n",
    "#     network = input_data(shape=[None, 224, 224, 1])\n",
    "#     conv_1 = relu(conv_2d(network, 64, 7, strides=2, bias=True, padding=padding, activation=None, name='Conv2d_1'))\n",
    "#     maxpool_1 = batch_normalization(max_pool_2d(conv_1, 3, strides=2, padding=padding, name='MaxPool_1'))\n",
    "#     LRN_1 = local_response_normalization(maxpool_1, name='LRN_1')\n",
    "#     # FeatEX-1\n",
    "#     conv_2a = relu(conv_2d(maxpool_1, 96, 1, strides=1, padding=padding, name='Conv_2a_FX1'))\n",
    "#     maxpool_2a = max_pool_2d(maxpool_1, 3, strides=1, padding=padding, name='MaxPool_2a_FX1')\n",
    "#     conv_2b = relu(conv_2d(conv_2a, 208, 3, strides=1, padding=padding, name='Conv_2b_FX1'))\n",
    "#     conv_2c = relu(conv_2d(maxpool_2a, 64, 1, strides=1, padding=padding, name='Conv_2c_FX1'))\n",
    "#     FX1_out = merge([conv_2b, conv_2c], mode='concat', axis=3, name='FX1_out')\n",
    "#     # FeatEX-2\n",
    "#     conv_3a = relu(conv_2d(FX1_out, 96, 1, strides=1, padding=padding, name='Conv_3a_FX2'))\n",
    "#     maxpool_3a = max_pool_2d(FX1_out, 3, strides=1, padding=padding, name='MaxPool_3a_FX2')\n",
    "#     conv_3b = relu(conv_2d(conv_3a, 208, 3, strides=1, padding=padding, name='Conv_3b_FX2'))\n",
    "#     conv_3c = relu(conv_2d(maxpool_3a, 64, 1, strides=1, padding=padding, name='Conv_3c_FX2'))\n",
    "#     FX2_out = merge([conv_3b, conv_3c], mode='concat', axis=3, name='FX2_out')\n",
    "#     net = flatten(FX2_out)\n",
    "#     if dropout:\n",
    "#         net = dropout(net, dropout_keep_prob)\n",
    "#     loss = fully_connected(net, num_classes,activation='softmax')\n",
    "\n",
    "#     # Compile the model and define the hyperparameters\n",
    "#     network = tflearn.regression(loss, optimizer='Adam',\n",
    "#                          loss='categorical_crossentropy',\n",
    "#                          learning_rate=0.0001)\n",
    "#     return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = create_Dexpression_network(dropout_keep_prob)\n",
    "\n",
    "#create a custom tensorflow session to manage the used resources\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config = config)\n",
    "\n",
    "\n",
    "# Final definition of model checkpoints and other configurations\n",
    "#model = tflearn.DNN(network, checkpoint_path='/home/cc/DeXpression/DeXpression_checkpoints',\n",
    "model = tflearn.DNN(network, checkpoint_path=tf_checkpoints,\n",
    "                    max_checkpoints=1, tensorboard_verbose=2, tensorboard_dir=\"./tflearn_logs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.RunOptions(report_tensor_allocations_upon_oom = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the model, train for 20 epochs. (Change all parameters to flags (arguments) on version 2.)\n",
    "#model.fit(X, Y, n_epoch=20, validation_set=0.1, shuffle=True, show_metric=True, batch_size=50, snapshot_step=2000,snapshot_epoch=True, run_id=RUNID)\n",
    "model.fit(X, Y, n_epoch=20, validation_set=(X_val,Y_val), shuffle=True, show_metric=True, batch_size=50, snapshot_step=2000,snapshot_epoch=True, run_id=RUNID)\n",
    "\n",
    "# Save the model\n",
    "model.save(tf_checkpoints + '/' + RUNID + '.model')\n",
    "print(\"finished training and saving\")\n",
    "\n",
    "# Load the model if required, later.\n",
    "#model.load('./DeXpression_checkpoints/' + RUNID + '.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
