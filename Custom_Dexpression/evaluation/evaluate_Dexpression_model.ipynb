{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found path with os.path.abspath('..'):  G:\\Documenten\\personal\\school\\MaNaMA_AI\\thesis\\implementation\\dexpression\\github_1\\github\\Thesis_Christiaan_Jean\\Custom_Dexpression\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "Lpath = os.path.abspath('..')\n",
    "print(\"found path with os.path.abspath('..'): \", Lpath)\n",
    "sys.path.insert(0, Lpath)\n",
    "\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tflearn.activations as activations\n",
    "# Data loading and preprocessing\n",
    "from tflearn.activations import relu\n",
    "from tflearn.data_utils import shuffle, to_categorical\n",
    "from tflearn.layers.conv import avg_pool_2d, conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import dropout, flatten, fully_connected, input_data\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.normalization import batch_normalization\n",
    "\n",
    "\n",
    "#chris library imports\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from test_recursive_image_load_V2 import load_CKP_data\n",
    "from test_recursive_image_load_V2 import load_formated_data\n",
    "from test_recursive_image_load_V2 import split_dataset\n",
    "from test_recursive_image_load_V2 import divide_subjects\n",
    "from test_recursive_image_load_V2 import divide_data_to_subject\n",
    "from test_recursive_image_load_V2 import opti_divide_data_to_subject\n",
    "from test_recursive_image_load_V2 import load_npy_files\n",
    "\n",
    "from showNumpyInfo import showInfo\n",
    "\n",
    "from Dexpression_network import create_Dexpression_old_network\n",
    "\n",
    "from confusion_matrix import plot_conf_mat\n",
    "from confusion_matrix import plot_norm_conf_mat\n",
    "from confusion_matrix import extract_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] could not load from path: ./data/CKP_all \n",
      " because of: [Errno 2] No such file or directory: './data/CKP_all/X.npy' error\n",
      "[WARNING] could not load from path: ./../data/CKP_all \n",
      " because of: [Errno 2] No such file or directory: './../data/CKP_all/X.npy' error\n",
      "[SUCCEED]data found in path:  ./../../data/CKP_all\n"
     ]
    }
   ],
   "source": [
    "# global Paths to define for each specific computer\n",
    "#tf_checkpoints = where the checkpoints of tensorflow training algorithms are stored to be recovered if necessary\n",
    "tf_checkpoints = \"G:/Documenten/personal/school/MaNaMA_AI/thesis/implementation/dexpression/github_1/github/Thesis_Christiaan_Jean/Custom_Dexpression/tf_checkpoints\"\n",
    "\n",
    "#load data\n",
    "# try:\n",
    "#     X_data = np.load('../data/CKP_X.npy')\n",
    "#     Y_data = np.load('../data/CKP_Y.npy')\n",
    "#     X_subID = (np.load('../data/CKP_subjectIds.npy')).astype('uint8')\n",
    "# except:\n",
    "#     X_data = np.load('G:/Documenten/personal/school/MaNaMA_AI/thesis/implementation/dexpression/github_1/github/Thesis_Christiaan_Jean/data/CKP_X.npy')\n",
    "#     Y_data = np.load('G:/Documenten/personal/school/MaNaMA_AI/thesis/implementation/dexpression/github_1/github/Thesis_Christiaan_Jean/data/CKP_Y.npy')\n",
    "#     X_subID = (np.load('G:/Documenten/personal/school/MaNaMA_AI/thesis/implementation/dexpression/github_1/github/Thesis_Christiaan_Jean/data/CKP_subjectIds.npy')).astype('uint8')\n",
    "dire = 'CKP_all'\n",
    "\n",
    "[X_data,Y_data,X_subID] = load_npy_files(5,dire)\n",
    "\n",
    "#load the subject distribution over the different datasets\n",
    "subID = (np.load('../data_division/'+dire+'/train_subject_ID.npy')).astype('uint8')\n",
    "subID_val = (np.load('../data_division/'+dire+'/validation_subject_ID.npy')).astype('uint8')\n",
    "subID_test = (np.load('../data_division/'+dire+'/test_subject_ID.npy')).astype('uint8')\n",
    "subIDs = [subID, subID_val, subID_test]\n",
    "\n",
    "\n",
    "# [X,Y,X_val,Y_val,X_test,Y_test] = divide_data_to_subject([X_data,Y_data,X_subID],subIDs)\n",
    "\n",
    "# print(X.shape)\n",
    "# print(X.dtype)\n",
    "\n",
    "# def format_data(divided_data):\n",
    "#     X = (divided_data[0].reshape(-1,224,224,1)).astype('uint8')\n",
    "#     Y = (divided_data[1].reshape(-1,7)).astype('uint8')\n",
    "\n",
    "#     # create the validation set X_val and Y-val (SubID_val is not given to the network)\n",
    "#     X_val = divided_data[2].reshape(-1,224,224,1).astype('uint8')\n",
    "#     Y_val = divided_data[3].reshape(-1,7).astype('uint8')\n",
    "\n",
    "#     # create the test set X_test and Y_test (SubID_test is not given to the network)\n",
    "#     X_test = divided_data[4].reshape(-1,224,224,1).astype('uint8')\n",
    "#     Y_test = divided_data[5].reshape(-1,7).astype('uint8')  \n",
    "    \n",
    "#     return [X,Y,X_val,Y_val,X_test,Y_test]\n",
    "\n",
    "# [X,Y,X_val,Y_val,X_test,Y_test] = format_data(divided_data)\n",
    "# print(X.shape)\n",
    "# print(X.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (1141,)\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (610,)\n",
      "Name  UnKnown\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (207,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "showInfo(subID)\n",
    "showInfo(subID_val)\n",
    "showInfo(subID_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more efficient version of the above\n",
    "# divides the data in training, validation and test sets according to lists of the subjectIDs already divided over the 3\n",
    "# IN:\n",
    "# data = contain 3 1D-arrays: x,y and subject data [X_data, Y_data,X_subID]\n",
    "# subIDs = contain 3 1D-arrays with the subject numbers for each set: train,val,test [subID subID_val subID_test]\n",
    "# OUT:\n",
    "# list of 6 arrays [X,Y,X_val,Y_val,X_test,Y_test]\n",
    "def opti_divide_data_to_subject(data,subIDs):\n",
    "    X_data = data[0]\n",
    "    Y_data = data[1]\n",
    "    X_subID = data[2]\n",
    "    \n",
    "    subID     = np.unique(subIDs[0])\n",
    "    subID_val = np.unique(subIDs[1])\n",
    "    subID_test= np.unique(subIDs[2])\n",
    "    \n",
    "    X = np.zeros((subIDs[0].shape[0]+10,224,224,1))\n",
    "    Y = np.zeros((subIDs[0].shape[0]+10,7))\n",
    "    X_val = np.zeros((subIDs[1].shape[0]+10,224,224,1))\n",
    "    Y_val = np.zeros((subIDs[1].shape[0]+10,7))\n",
    "    X_test = np.zeros((subIDs[2].shape[0]+10,224,224,1))\n",
    "    Y_test = np.zeros((subIDs[2].shape[0]+10,7))\n",
    "\n",
    "    index = 0\n",
    "    index_val = 0\n",
    "    index_test = 0\n",
    "\n",
    "    showInfo(subID)\n",
    "    showInfo(subID_val)\n",
    "    showInfo(subID_test)\n",
    "#     extract images and labels belonging to the trainings list of subject IDs in subID\n",
    "    cumul = 0\n",
    "    count = 0\n",
    "    for i in subID :\n",
    "        count = count + 1\n",
    "        print(\"subID count\", count)\n",
    "        print(\"subID \", i)\n",
    "        same =np.argwhere(X_subID==i)\n",
    "        print(\"same \",same)\n",
    "        print(\"same \",same.shape[0])\n",
    "        cumul = cumul + same.shape[0]\n",
    "        print(\"same cumul\",cumul)\n",
    "        for j in same:\n",
    "            print(j[0])\n",
    "            X[index] = X_data[j[0]]\n",
    "            Y[index] = Y_data[j[0]]\n",
    "            index =  index + 1\n",
    "    print(\"index is \", index)\n",
    "\n",
    "#     extract images and labels belonging to the validation list of subject IDs in subID_val            \n",
    "    for i in subID_val :\n",
    "        for j in np.argwhere(X_subID==i):\n",
    "#             print(j[0])\n",
    "            X_val[index_val]  = X_data[j[0]]\n",
    "            Y_val[index_val]  = Y_data[j[0]]\n",
    "            index_val =  index_val + 1\n",
    "    print (\"index_val is \", index_val)\n",
    " #     extract images and labels belonging to the test list of subject IDs in subID_test           \n",
    "    for i in subID_test :\n",
    "        for j in np.argwhere(X_subID==i):\n",
    "#             print(j[0])\n",
    "            X_test[index_test]  = X_data[j[0]]\n",
    "            Y_test[index_test]  = Y_data[j[0]]\n",
    "            index_test =  index_test + 1\n",
    "    print(\"index_test is \", index_test)\n",
    "            \n",
    "    X = (X.reshape(-1,224,224,1)).astype('uint8')\n",
    "    Y = (Y.reshape(-1,7)).astype('uint8')\n",
    "\n",
    "    # create the validation set X_val and Y-val (SubID_val is not given to the network)\n",
    "    X_val = X_val.reshape(-1,224,224,1).astype('uint8')\n",
    "    Y_val = Y_val.reshape(-1,7).astype('uint8')\n",
    "\n",
    "    # create the test set X_test and Y_test (SubID_test is not given to the network)\n",
    "    X_test = X_test.reshape(-1,224,224,1).astype('uint8')\n",
    "    Y_test = Y_test.reshape(-1,7).astype('uint8')\n",
    "\n",
    "    return [X,Y,X_val,Y_val,X_test,Y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divides the data in training, validation and test sets according to lists of the subjectIDs already divided over the 3\n",
    "# IN:\n",
    "# data = contain 3 1D-arrays: x,y and subject data [X_data, Y_data,X_subID]\n",
    "# subIDs = contain 3 1D-arrays with the subject numbers for each set: train,val,test [subID subID_val subID_test]\n",
    "# OUT:\n",
    "# list of 6 arrays [X,Y,X_val,Y_val,X_test,Y_test]\n",
    "def divide_data_to_subject(data,subIDs):\n",
    "    X_data = data[0]\n",
    "    Y_data = data[1]\n",
    "    X_subID = data[2]\n",
    "    \n",
    "    subID     = np.unique(subIDs[0])\n",
    "    subID_val = np.unique(subIDs[1])\n",
    "    subID_test= np.unique(subIDs[2])\n",
    "    \n",
    "    showInfo(subIDs[0],'subIDs[0]')\n",
    "    showInfo(subIDs[1],'subIDs[1]')\n",
    "    showInfo(subIDs[2],'subIDs[3]')\n",
    "    \n",
    "    showInfo(subID,'subID')\n",
    "    showInfo(subID_val,'subID_val')\n",
    "    showInfo(subID_test, 'subID_test')\n",
    "    \n",
    "    X = np.asarray([])\n",
    "    Y = np.asarray([]) \n",
    "    X_val = np.asarray([])\n",
    "    Y_val = np.asarray([])\n",
    "    X_test = np.asarray([])\n",
    "    Y_test = np.asarray([])\n",
    "\n",
    "#     extract images and labels belonging to the trainings list of subject IDs in subID\n",
    "    for i in subID :\n",
    "        for j in np.argwhere(X_subID==i):\n",
    "#             print(j[0])\n",
    "            X = np.append(X,X_data[j[0]])\n",
    "            Y = np.append(Y,Y_data[j[0]])\n",
    "#     extract images and labels belonging to the validation list of subject IDs in subID_val            \n",
    "    for i in subID_val :\n",
    "        for j in np.argwhere(X_subID==i):\n",
    "#             print(j[0])\n",
    "            X_val  = np.append(X_val,X_data[j[0]])\n",
    "            Y_val  = np.append(Y_val,Y_data[j[0]]) \n",
    " #     extract images and labels belonging to the test list of subject IDs in subID_test           \n",
    "    for i in subID_test :\n",
    "        for j in np.argwhere(X_subID==i):\n",
    "#             print(j[0])\n",
    "            X_test = np.append(X_test,X_data[j[0]])\n",
    "            Y_test = np.append(Y_test,Y_data[j[0]])\n",
    "            \n",
    "    X = (X.reshape(-1,224,224,1)).astype('uint8')\n",
    "    Y = (Y.reshape(-1,7)).astype('uint8')\n",
    "\n",
    "    # create the validation set X_val and Y-val (SubID_val is not given to the network)\n",
    "    X_val = X_val.reshape(-1,224,224,1).astype('uint8')\n",
    "    Y_val = Y_val.reshape(-1,7).astype('uint8')\n",
    "\n",
    "    # create the test set X_test and Y_test (SubID_test is not given to the network)\n",
    "    X_test = X_test.reshape(-1,224,224,1).astype('uint8')\n",
    "    Y_test = Y_test.reshape(-1,7).astype('uint8')\n",
    "    \n",
    "    showInfo(X,'X')\n",
    "    showInfo(Y,'Y')\n",
    "    showInfo(X_val,'X_val')\n",
    "    showInfo(Y_val,'Y_val')\n",
    "    showInfo(X_test,'X_test')\n",
    "    showInfo(Y_test,'Y_test')\n",
    "    return [X,Y,X_val,Y_val,X_test,Y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name  subIDs[0]\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (1141,)\n",
      "Name  subIDs[1]\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (610,)\n",
      "Name  subIDs[3]\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (207,)\n",
      "Name  subID\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (73,)\n",
      "Name  subID_val\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (34,)\n",
      "Name  subID_test\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (11,)\n",
      "Name  X\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (1158, 224, 224, 1)\n",
      "Name  Y\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (1158, 7)\n",
      "Name  X_val\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (612, 224, 224, 1)\n",
      "Name  Y_val\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (612, 7)\n",
      "Name  X_test\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (207, 224, 224, 1)\n",
      "Name  Y_test\n",
      "type:  <class 'numpy.ndarray'>\n",
      "Dtype:  uint8\n",
      "shape:  (207, 7)\n"
     ]
    }
   ],
   "source": [
    "# divided_data_opti = opti_divide_data_to_subject([X_data,Y_data,X_subID],subIDs)\n",
    "\n",
    "divided_data = divide_data_to_subject([X_data,Y_data,X_subID],subIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network = create_Dexpression_old_network()\n",
    "\n",
    "#create a custom tensorflow session to manage the used resources\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = tf.Session(config = config)\n",
    "\n",
    "\n",
    "# Final definition of model checkpoints and other configurations\n",
    "#model = tflearn.DNN(network, checkpoint_path='/home/cc/DeXpression/DeXpression_checkpoints',\n",
    "# model = tflearn.DNN(network, checkpoint_path=tf_checkpoints,\n",
    "#                     max_checkpoints=1, tensorboard_verbose=2, tensorboard_dir=\"./tflearn_logs/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load('./tf_checkpoints/DeXpression_run_3.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Y_pred_classes(data,model):\n",
    "    [X,Y,X_val,Y_val,X_test,Y_test] = data\n",
    "    # predictions \n",
    "    Y_val_pred = model.predict(X_val)  # predictions\n",
    "    Y_test_pred = model.predict(X_test)  # predictions\n",
    "\n",
    "    # Y_train_pred_class = extract_classes(model.predict(X))\n",
    "\n",
    "    Y_train_pred_class = extract_classes(model.predict(X[:50]))\n",
    "    Y_train_pred_class = np.append(Y_train_pred_class,extract_classes(model.predict(X[50:100])))\n",
    "    Y_train_pred_class = np.append(Y_train_pred_class,extract_classes(model.predict(X[100:150])))\n",
    "    Y_train_pred_class = np.append(Y_train_pred_class,extract_classes(model.predict(X[150:200])))\n",
    "    Y_train_pred_class = np.append(Y_train_pred_class,extract_classes(model.predict(X[200:250])))\n",
    "    Y_train_pred_class = np.append(Y_train_pred_class,extract_classes(model.predict(X[250:])))\n",
    "\n",
    "    Y_train_class = extract_classes(Y)\n",
    "\n",
    "    # extracting classes\n",
    "    Y_val_pred_class = extract_classes(Y_val_pred)\n",
    "    Y_val_class = extract_classes(Y_val)\n",
    "\n",
    "    Y_test_pred_class = extract_classes(Y_test_pred)\n",
    "    Y_test_class = extract_classes(Y_test)\n",
    "\n",
    "    print(Y_train_class.astype('uint8'))\n",
    "    print(Y_train_pred_class.astype('uint8'))\n",
    "    \n",
    "    # making full dataset\n",
    "    Y_pred_class = Y_train_pred_class\n",
    "    Y_pred_class = np.append(Y_pred_class,Y_val_pred_class)\n",
    "    Y_pred_class = np.append(Y_pred_class,Y_test_pred_class) \n",
    "\n",
    "    Y_class = np.append(Y_train_class,Y_val_class)\n",
    "    Y_class = np.append(Y_class,Y_test_class)\n",
    "    \n",
    "    return [Y_train_class, Y_train_pred_class,\n",
    "            Y_val_class, Y_val_pred_class, \n",
    "            Y_test_class, Y_test_pred_class, \n",
    "            Y_class, Y_pred_class   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Y_train_class, Y_train_pred_class, Y_val_class, Y_val_pred_class, Y_test_class, Y_test_pred_class, Y_class, Y_pred_class   ] = create_Y_pred_classes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#calculate global accuracy\n",
    "#sum of all correct (top-1) classification divide by the sum off all predictions\n",
    "# IN:\n",
    "# Y_class: Np.ndarray (Nx1) correct class labels  \n",
    "# Y_pred_class: Np.ndarray (Nx1) predicted class labels \n",
    "# OUT:\n",
    "# accuracy value (float64)\n",
    "def global_acc(Y_class,Y_pred_class,verbose = 0):\n",
    "    Y_class_corr = np.asarray([]) \n",
    "    for i in range(0,Y_pred_class.astype('uint8').shape[0]):\n",
    "        if(verbose >=2):\n",
    "            print('Y_class[ ',i,'] ' ,Y_class[i])\n",
    "            print('Y_pred_class[ ',i,'] ',Y_pred_class[i])\n",
    "            print(Y_pred_class[i] == Y_class[i])\n",
    "        if(Y_pred_class[i] == Y_class[i]):\n",
    "            Y_class_corr = np.append(Y_class_corr,1)\n",
    "        else:\n",
    "            Y_class_corr = np.append(Y_class_corr,0)\n",
    "                        \n",
    "    Y_pred_class_count = np.bincount(Y_class_corr.astype('uint8'),minlength=2)\n",
    "    acc = Y_pred_class_count.astype('float64')[1]/(Y_pred_class_count.astype('float64').sum())\n",
    "    \n",
    "    if(verbose >=1):\n",
    "        print(Y_class_corr)\n",
    "        print(Y_pred_class_count)\n",
    "        print(acc)\n",
    "        \n",
    "    return acc\n",
    "\n",
    "#TEST\n",
    "\n",
    "# Y_t_class = np.arange(10)\n",
    "# print(Y_t_class)\n",
    "# Y_t_predict = np.asarray([0,1,2,4,4,5,7,7,8,9]) \n",
    "# print(Y_t_predict)\n",
    "# print(global_acc(Y_t_class,Y_t_predict,0))\n",
    "def display_all_acc(classes,modelID = 'NaN'):\n",
    "    print(\"model_\",modelID)\n",
    "    [   Y_train_class, Y_train_pred_class,\n",
    "                        Y_val_class, Y_val_pred_class, \n",
    "                        Y_test_class, Y_test_pred_class, \n",
    "                        Y_class, Y_pred_class   ] = classes\n",
    "    \n",
    "    \n",
    "    glob_acc = global_acc(Y_class,Y_pred_class)\n",
    "    train_acc = global_acc(Y_train_class,Y_train_pred_class)\n",
    "    val_acc = global_acc(Y_val_class,Y_val_pred_class)\n",
    "    test_acc = global_acc(Y_test_class,Y_test_pred_class)\n",
    "\n",
    "    print(\"train accuracy     : \", train_acc)\n",
    "    print(\"validation accuracy: \", val_acc)\n",
    "    print(\"test accuracy      : \", test_acc)\n",
    "    print(\"global accuracy    : \", glob_acc)\n",
    "    \n",
    "# display_all_acc([Y_train_class, Y_train_pred_class, Y_val_class, Y_val_pred_class, Y_test_class, Y_test_pred_class, Y_class, Y_pred_class   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bin_count(Y_class,title = 'no title',size=(20,14)):\n",
    "    x = ['Anger','Contempt','Disgust','Fear','Happy','Saddness','Surprise']\n",
    "    y = np.bincount(Y_class.astype('uint8'),minlength=7)\n",
    "    plt.figure(figsize=size)\n",
    "    plt.bar(x,y)\n",
    "    plt.title(title)\n",
    "    for i, v in enumerate(y):\n",
    "        plt.text(i - size[0]/160, v  + size[1]/40  , str(v), color='black', fontweight='bold')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_conf_mat(classes,modelID = 'NaN'):\n",
    "    [   Y_train_class, Y_train_pred_class,\n",
    "                        Y_val_class, Y_val_pred_class, \n",
    "                        Y_test_class, Y_test_pred_class, \n",
    "                        Y_class, Y_pred_class   ] = classes\n",
    "    \n",
    "    \n",
    "    \n",
    "    conf_mat_train = tf.confusion_matrix(Y_train_class,Y_train_pred_class,num_classes=7,dtype=tf.int32)#,# name=None,# weights=None)\n",
    "    conf_mat_val = tf.confusion_matrix(Y_val_class,Y_val_pred_class,num_classes=7,dtype=tf.int32)#,# name=None,# weights=None)\n",
    "    conf_mat_test = tf.confusion_matrix(Y_test_class,Y_test_pred_class,num_classes=7,dtype=tf.int32)#,# name=None,# weights=None)\n",
    "    conf_mat = tf.confusion_matrix(Y_class,Y_pred_class,num_classes=7,dtype=tf.int32)#,# name=None,# weights=None)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    plot_conf_mat(sess.run(conf_mat_train),('training_model_',modelID),(10,7))\n",
    "    plot_conf_mat(sess.run(conf_mat_val),('validation_model_',modelID),(10,7))\n",
    "    plot_conf_mat(sess.run(conf_mat_test),('test_model_',modelID),(10,7))\n",
    "    plot_conf_mat(sess.run(conf_mat),('full_model_',modelID),(10,7))\n",
    "    \n",
    "    plot_norm_conf_mat(sess.run(conf_mat_train),('training_normalised_model_',modelID),(10,7))\n",
    "    plot_bin_count(Y_train_class,('training set class distribution model_',modelID),(10,7))\n",
    "    \n",
    "    plot_norm_conf_mat(sess.run(conf_mat_val),('validation_normalised_model_',modelID),(10,7))\n",
    "    plot_bin_count(Y_val_class,('validation set class distribution',modelID),(10,7))\n",
    "        \n",
    "    plot_norm_conf_mat(sess.run(conf_mat_test),('test_normalised_model_',modelID),(10,7))\n",
    "    plot_bin_count(Y_test_class,('test set class distribution model_',modelID),(10,7))\n",
    "        \n",
    "    plot_norm_conf_mat(sess.run(conf_mat),('full_normalised_model_',modelID),(10,7))\n",
    "    plot_bin_count(Y_class,('full set class distribution model_',modelID),(10,7))\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_model_analysis(data,model,modelID = \"NaN\"):\n",
    "    classes = create_Y_pred_classes(data,model)\n",
    "    display_all_acc(classes,modelID)\n",
    "    all_conf_mat(classes,modelID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a custom tensorflow session to manage the used resources\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config = config)\n",
    "\n",
    "\n",
    "model1 = tflearn.DNN(network, checkpoint_path=tf_checkpoints,\n",
    "                    max_checkpoints=1, tensorboard_verbose=2, tensorboard_dir=\"./tflearn_logs/\")\n",
    "# model3 = tflearn.DNN(network, checkpoint_path=tf_checkpoints,\n",
    "#                     max_checkpoints=1, tensorboard_verbose=2, tensorboard_dir=\"./tflearn_logs/\")\n",
    "\n",
    "# model4 = tflearn.DNN(network, checkpoint_path=tf_checkpoints,\n",
    "#                     max_checkpoints=1, tensorboard_verbose=2, tensorboard_dir=\"./tflearn_logs/\")\n",
    "\n",
    "# model5 = tflearn.DNN(network, checkpoint_path=tf_checkpoints,\n",
    "#                     max_checkpoints=1, tensorboard_verbose=2, tensorboard_dir=\"./tflearn_logs/\")\n",
    "\n",
    "\n",
    "\n",
    "# model1.load('./tf_checkpoints/DeXpression_run_1.model')\n",
    "# model3.load('./tf_checkpoints/DeXpression_run_3.model')\n",
    "# model4.load('./tf_checkpoints/DeXpression_run_4.model')\n",
    "# model5.load('./tf_checkpoints/DeXpression_run_5.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "# classes = create_Y_pred_classes(divided_data,model1)\n",
    "# print(classes[0])\n",
    "# print(classes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.load('../tf_checkpoints/DeXpression_run_1.model')\n",
    "full_model_analysis(divided_data,model1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.load('../tf_checkpoints/DeXpression_run_3.model')\n",
    "full_model_analysis(divided_data,model1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1.load('../tf_checkpoints/DeXpression_run_4.model')\n",
    "full_model_analysis(divided_data,model1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.load('../tf_checkpoints/DeXpression_run_5.model')\n",
    "full_model_analysis(divided_data,model1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_model_analysis(divided_data,model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
